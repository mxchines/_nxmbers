[
    {
        "label": "rpy2.robjects",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "rpy2.robjects",
        "description": "rpy2.robjects",
        "detail": "rpy2.robjects",
        "documentation": {}
    },
    {
        "label": "pandas2ri",
        "importPath": "rpy2.robjects",
        "description": "rpy2.robjects",
        "isExtraImport": true,
        "detail": "rpy2.robjects",
        "documentation": {}
    },
    {
        "label": "importr",
        "importPath": "rpy2.robjects.packages",
        "description": "rpy2.robjects.packages",
        "isExtraImport": true,
        "detail": "rpy2.robjects.packages",
        "documentation": {}
    },
    {
        "label": "localconverter",
        "importPath": "rpy2.robjects.conversion",
        "description": "rpy2.robjects.conversion",
        "isExtraImport": true,
        "detail": "rpy2.robjects.conversion",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "socket",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "socket",
        "description": "socket",
        "detail": "socket",
        "documentation": {}
    },
    {
        "label": "plotly.graph_objects",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "plotly.graph_objects",
        "description": "plotly.graph_objects",
        "detail": "plotly.graph_objects",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "get_data_path",
        "importPath": "path_utility",
        "description": "path_utility",
        "isExtraImport": true,
        "detail": "path_utility",
        "documentation": {}
    },
    {
        "label": "get_data_path",
        "importPath": "path_utility",
        "description": "path_utility",
        "isExtraImport": true,
        "detail": "path_utility",
        "documentation": {}
    },
    {
        "label": "get_data_path",
        "importPath": "path_utility",
        "description": "path_utility",
        "isExtraImport": true,
        "detail": "path_utility",
        "documentation": {}
    },
    {
        "label": "get_data_path",
        "importPath": "path_utility",
        "description": "path_utility",
        "isExtraImport": true,
        "detail": "path_utility",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "TimeSeries",
        "importPath": "alpha_vantage.timeseries",
        "description": "alpha_vantage.timeseries",
        "isExtraImport": true,
        "detail": "alpha_vantage.timeseries",
        "documentation": {}
    },
    {
        "label": "gql",
        "importPath": "gql",
        "description": "gql",
        "isExtraImport": true,
        "detail": "gql",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "gql",
        "description": "gql",
        "isExtraImport": true,
        "detail": "gql",
        "documentation": {}
    },
    {
        "label": "RequestsHTTPTransport",
        "importPath": "gql.transport.requests",
        "description": "gql.transport.requests",
        "isExtraImport": true,
        "detail": "gql.transport.requests",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "config",
        "description": "config",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_TICKER",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_START_DATE",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_END_DATE",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_INTERVAL",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_API_SOURCES",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "BEAM_API_KEY",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_TICKER",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_START_DATE",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_END_DATE",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_INTERVAL",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_API_SOURCES",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "BEAM_API_KEY",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "clean_data",
        "importPath": "data_cleaner",
        "description": "data_cleaner",
        "isExtraImport": true,
        "detail": "data_cleaner",
        "documentation": {}
    },
    {
        "label": "psycopg2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psycopg2",
        "description": "psycopg2",
        "detail": "psycopg2",
        "documentation": {}
    },
    {
        "label": "sql",
        "importPath": "psycopg2",
        "description": "psycopg2",
        "isExtraImport": true,
        "detail": "psycopg2",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "main",
        "importPath": "data_ingestion.data_fetcher",
        "description": "data_ingestion.data_fetcher",
        "isExtraImport": true,
        "detail": "data_ingestion.data_fetcher",
        "documentation": {}
    },
    {
        "label": "clean_data",
        "importPath": "data_cleaning.data_cleaner",
        "description": "data_cleaning.data_cleaner",
        "isExtraImport": true,
        "detail": "data_cleaning.data_cleaner",
        "documentation": {}
    },
    {
        "label": "main",
        "importPath": "data_storage.rds_uploader",
        "description": "data_storage.rds_uploader",
        "isExtraImport": true,
        "detail": "data_storage.rds_uploader",
        "documentation": {}
    },
    {
        "label": "upload_to_rds",
        "importPath": "data_storage.rds_uploader",
        "description": "data_storage.rds_uploader",
        "isExtraImport": true,
        "detail": "data_storage.rds_uploader",
        "documentation": {}
    },
    {
        "label": "update_config_data",
        "importPath": "app",
        "description": "app",
        "isExtraImport": true,
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "main",
        "importPath": "data_ingestion.data_combiner",
        "description": "data_ingestion.data_combiner",
        "isExtraImport": true,
        "detail": "data_ingestion.data_combiner",
        "documentation": {}
    },
    {
        "label": "run_r_models",
        "importPath": "prediction_model.r_model_executor",
        "description": "prediction_model.r_model_executor",
        "isExtraImport": true,
        "detail": "prediction_model.r_model_executor",
        "documentation": {}
    },
    {
        "label": "save_results",
        "importPath": "results_storage.results_saver",
        "description": "results_storage.results_saver",
        "isExtraImport": true,
        "detail": "results_storage.results_saver",
        "documentation": {}
    },
    {
        "label": "base",
        "kind": 5,
        "importPath": "R.arima_model.R",
        "description": "R.arima_model.R",
        "peekOfCode": "base = importr('base')\nDBI = importr('DBI')\nRPostgres = importr('RPostgres')\nforecast = importr('forecast')\nstats = importr('stats')\n# Database connection parameters\ndb_host = \"numbermxchine.cxwoaq8ccu34.eu-west-2.rds.amazonaws.com\"\ndb_name = \"nxmbers\"\ndb_user = \"mxchinist\"\ndb_password = \"foJzyn-miwhor-bavpo4\"",
        "detail": "R.arima_model.R",
        "documentation": {}
    },
    {
        "label": "DBI",
        "kind": 5,
        "importPath": "R.arima_model.R",
        "description": "R.arima_model.R",
        "peekOfCode": "DBI = importr('DBI')\nRPostgres = importr('RPostgres')\nforecast = importr('forecast')\nstats = importr('stats')\n# Database connection parameters\ndb_host = \"numbermxchine.cxwoaq8ccu34.eu-west-2.rds.amazonaws.com\"\ndb_name = \"nxmbers\"\ndb_user = \"mxchinist\"\ndb_password = \"foJzyn-miwhor-bavpo4\"\ndb_port = 5432",
        "detail": "R.arima_model.R",
        "documentation": {}
    },
    {
        "label": "RPostgres",
        "kind": 5,
        "importPath": "R.arima_model.R",
        "description": "R.arima_model.R",
        "peekOfCode": "RPostgres = importr('RPostgres')\nforecast = importr('forecast')\nstats = importr('stats')\n# Database connection parameters\ndb_host = \"numbermxchine.cxwoaq8ccu34.eu-west-2.rds.amazonaws.com\"\ndb_name = \"nxmbers\"\ndb_user = \"mxchinist\"\ndb_password = \"foJzyn-miwhor-bavpo4\"\ndb_port = 5432\ntry:",
        "detail": "R.arima_model.R",
        "documentation": {}
    },
    {
        "label": "forecast",
        "kind": 5,
        "importPath": "R.arima_model.R",
        "description": "R.arima_model.R",
        "peekOfCode": "forecast = importr('forecast')\nstats = importr('stats')\n# Database connection parameters\ndb_host = \"numbermxchine.cxwoaq8ccu34.eu-west-2.rds.amazonaws.com\"\ndb_name = \"nxmbers\"\ndb_user = \"mxchinist\"\ndb_password = \"foJzyn-miwhor-bavpo4\"\ndb_port = 5432\ntry:\n    # Resolve hostname to IP address",
        "detail": "R.arima_model.R",
        "documentation": {}
    },
    {
        "label": "stats",
        "kind": 5,
        "importPath": "R.arima_model.R",
        "description": "R.arima_model.R",
        "peekOfCode": "stats = importr('stats')\n# Database connection parameters\ndb_host = \"numbermxchine.cxwoaq8ccu34.eu-west-2.rds.amazonaws.com\"\ndb_name = \"nxmbers\"\ndb_user = \"mxchinist\"\ndb_password = \"foJzyn-miwhor-bavpo4\"\ndb_port = 5432\ntry:\n    # Resolve hostname to IP address\n    logging.info(f\"Resolving hostname: {db_host}\")",
        "detail": "R.arima_model.R",
        "documentation": {}
    },
    {
        "label": "db_host",
        "kind": 5,
        "importPath": "R.arima_model.R",
        "description": "R.arima_model.R",
        "peekOfCode": "db_host = \"numbermxchine.cxwoaq8ccu34.eu-west-2.rds.amazonaws.com\"\ndb_name = \"nxmbers\"\ndb_user = \"mxchinist\"\ndb_password = \"foJzyn-miwhor-bavpo4\"\ndb_port = 5432\ntry:\n    # Resolve hostname to IP address\n    logging.info(f\"Resolving hostname: {db_host}\")\n    db_ip = socket.gethostbyname(db_host)\n    logging.info(f\"Resolved IP: {db_ip}\")",
        "detail": "R.arima_model.R",
        "documentation": {}
    },
    {
        "label": "db_name",
        "kind": 5,
        "importPath": "R.arima_model.R",
        "description": "R.arima_model.R",
        "peekOfCode": "db_name = \"nxmbers\"\ndb_user = \"mxchinist\"\ndb_password = \"foJzyn-miwhor-bavpo4\"\ndb_port = 5432\ntry:\n    # Resolve hostname to IP address\n    logging.info(f\"Resolving hostname: {db_host}\")\n    db_ip = socket.gethostbyname(db_host)\n    logging.info(f\"Resolved IP: {db_ip}\")\n    # Connect to the database",
        "detail": "R.arima_model.R",
        "documentation": {}
    },
    {
        "label": "db_user",
        "kind": 5,
        "importPath": "R.arima_model.R",
        "description": "R.arima_model.R",
        "peekOfCode": "db_user = \"mxchinist\"\ndb_password = \"foJzyn-miwhor-bavpo4\"\ndb_port = 5432\ntry:\n    # Resolve hostname to IP address\n    logging.info(f\"Resolving hostname: {db_host}\")\n    db_ip = socket.gethostbyname(db_host)\n    logging.info(f\"Resolved IP: {db_ip}\")\n    # Connect to the database\n    logging.info(\"Connecting to the database...\")",
        "detail": "R.arima_model.R",
        "documentation": {}
    },
    {
        "label": "db_password",
        "kind": 5,
        "importPath": "R.arima_model.R",
        "description": "R.arima_model.R",
        "peekOfCode": "db_password = \"foJzyn-miwhor-bavpo4\"\ndb_port = 5432\ntry:\n    # Resolve hostname to IP address\n    logging.info(f\"Resolving hostname: {db_host}\")\n    db_ip = socket.gethostbyname(db_host)\n    logging.info(f\"Resolved IP: {db_ip}\")\n    # Connect to the database\n    logging.info(\"Connecting to the database...\")\n    con = DBI.dbConnect(RPostgres.Postgres(),",
        "detail": "R.arima_model.R",
        "documentation": {}
    },
    {
        "label": "db_port",
        "kind": 5,
        "importPath": "R.arima_model.R",
        "description": "R.arima_model.R",
        "peekOfCode": "db_port = 5432\ntry:\n    # Resolve hostname to IP address\n    logging.info(f\"Resolving hostname: {db_host}\")\n    db_ip = socket.gethostbyname(db_host)\n    logging.info(f\"Resolved IP: {db_ip}\")\n    # Connect to the database\n    logging.info(\"Connecting to the database...\")\n    con = DBI.dbConnect(RPostgres.Postgres(),\n                        host=db_ip,",
        "detail": "R.arima_model.R",
        "documentation": {}
    },
    {
        "label": "kalman_filter",
        "kind": 2,
        "importPath": "R.kalman_filter.R",
        "description": "R.kalman_filter.R",
        "peekOfCode": "def kalman_filter(data, initial_state, process_noise_variance, measurement_noise_variance):\n    \"\"\"\n    Implements a basic Kalman filter for time series data.\n    Args:\n        data: A 1D numpy array containing the time series data.\n        initial_state: A tuple (x0, P0) representing the initial state estimate and its covariance.\n        process_noise_variance: The variance of the process noise (Q).\n        measurement_noise_variance: The variance of the measurement noise (R).\n    Returns:\n        filtered_state_means: A 1D numpy array containing the filtered state estimates.",
        "detail": "R.kalman_filter.R",
        "documentation": {}
    },
    {
        "label": "initial_state",
        "kind": 5,
        "importPath": "R.kalman_filter.R",
        "description": "R.kalman_filter.R",
        "peekOfCode": "initial_state = (pdf['close_alpha'].iloc[0], 1.0)  # Initial state estimate and covariance\nprocess_noise_variance = 0.1  # Adjust based on your understanding of the process\nmeasurement_noise_variance = 1.0  # Adjust based on your understanding of the measurement noise\nfiltered_state_means, filtered_state_covariances = kalman_filter(\n    pdf['close_alpha'].values, initial_state, process_noise_variance, measurement_noise_variance\n)\n# Plot the filtered results\nlogging.info(\"Plotting Kalman filtered results...\")\nplt.figure(figsize=(12, 6))\nplt.plot(pdf['date'], pdf['close_alpha'], label='Actual')",
        "detail": "R.kalman_filter.R",
        "documentation": {}
    },
    {
        "label": "process_noise_variance",
        "kind": 5,
        "importPath": "R.kalman_filter.R",
        "description": "R.kalman_filter.R",
        "peekOfCode": "process_noise_variance = 0.1  # Adjust based on your understanding of the process\nmeasurement_noise_variance = 1.0  # Adjust based on your understanding of the measurement noise\nfiltered_state_means, filtered_state_covariances = kalman_filter(\n    pdf['close_alpha'].values, initial_state, process_noise_variance, measurement_noise_variance\n)\n# Plot the filtered results\nlogging.info(\"Plotting Kalman filtered results...\")\nplt.figure(figsize=(12, 6))\nplt.plot(pdf['date'], pdf['close_alpha'], label='Actual')\nplt.plot(pdf['date'], filtered_state_means, label='Kalman Filtered', color='green')",
        "detail": "R.kalman_filter.R",
        "documentation": {}
    },
    {
        "label": "measurement_noise_variance",
        "kind": 5,
        "importPath": "R.kalman_filter.R",
        "description": "R.kalman_filter.R",
        "peekOfCode": "measurement_noise_variance = 1.0  # Adjust based on your understanding of the measurement noise\nfiltered_state_means, filtered_state_covariances = kalman_filter(\n    pdf['close_alpha'].values, initial_state, process_noise_variance, measurement_noise_variance\n)\n# Plot the filtered results\nlogging.info(\"Plotting Kalman filtered results...\")\nplt.figure(figsize=(12, 6))\nplt.plot(pdf['date'], pdf['close_alpha'], label='Actual')\nplt.plot(pdf['date'], filtered_state_means, label='Kalman Filtered', color='green')\nplt.title('Kalman Filter Results')",
        "detail": "R.kalman_filter.R",
        "documentation": {}
    },
    {
        "label": "clean_data",
        "kind": 2,
        "importPath": "data_cleaning.data_cleaner",
        "description": "data_cleaning.data_cleaner",
        "peekOfCode": "def clean_data():\n    # Define the input directory\n    input_dir = os.path.join(os.path.dirname(__file__), '..', 'nxmbers', 'data', 'csv')\n    # Define the output directory\n    output_dir = os.path.join(os.path.dirname(__file__), '..', 'nxmbers', 'data', 'cleaned')\n    # Iterate over the CSV files in the input directory\n    for file in os.listdir(input_dir):\n        if file.endswith('.csv'):\n            # Construct the full file path\n            input_file_path = os.path.join(input_dir, file)",
        "detail": "data_cleaning.data_cleaner",
        "documentation": {}
    },
    {
        "label": "get_project_path",
        "kind": 2,
        "importPath": "data_cleaning.path_utility",
        "description": "data_cleaning.path_utility",
        "peekOfCode": "def get_project_path(file_path):\n    \"\"\"\n    Get the project path based on the current file's location.\n    :param file_path: The __file__ variable of the current script\n    :return: Path object pointing to the project root (nxmbers folder)\n    \"\"\"\n    current_file = Path(file_path).resolve()\n    project_root = current_file.parent\n    while project_root.name != 'nxmbers':\n        project_root = project_root.parent",
        "detail": "data_cleaning.path_utility",
        "documentation": {}
    },
    {
        "label": "get_data_path",
        "kind": 2,
        "importPath": "data_cleaning.path_utility",
        "description": "data_cleaning.path_utility",
        "peekOfCode": "def get_data_path(file_path, filename):\n    \"\"\"\n    Get the full path for a data file within the project.\n    :param file_path: The __file__ variable of the current script\n    :param filename: Name of the file to be saved/loaded\n    :return: Full path to the data file\n    \"\"\"\n    project_root = get_project_path(file_path)\n    data_dir = project_root / 'nxmbers' / 'data'\n    data_dir.mkdir(parents=True, exist_ok=True)",
        "detail": "data_cleaning.path_utility",
        "documentation": {}
    },
    {
        "label": "combine_data",
        "kind": 2,
        "importPath": "data_ingestion.data_combiner",
        "description": "data_ingestion.data_combiner",
        "peekOfCode": "def combine_data(yahoo_data, alpha_data):\n    \"\"\"\n    Combine Yahoo Finance and Alpha Vantage data.\n    \"\"\"\n    # Reset index for all dataframes\n    yahoo_data = yahoo_data.reset_index()\n    alpha_data = alpha_data.reset_index()\n    # Ensure consistent date column naming\n    yahoo_data = yahoo_data.rename(columns={'Date': 'date'})\n    alpha_data = alpha_data.rename(columns={'date': 'date'})",
        "detail": "data_ingestion.data_combiner",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "data_ingestion.data_combiner",
        "description": "data_ingestion.data_combiner",
        "peekOfCode": "def main(ticker):\n    yahoo_csv_path = get_data_path(__file__, f'{ticker}_yahoo_data.csv')\n    alpha_csv_path = get_data_path(__file__, f'{ticker}_alpha_data.csv')\n    yahoo_data = pd.read_csv(yahoo_csv_path, parse_dates=['date'])\n    alpha_data = pd.read_csv(alpha_csv_path, parse_dates=['date'])\n    combined_data = combine_data(yahoo_data, alpha_data)\n    combined_csv_path = get_data_path(__file__, f'{ticker}_combined_data.csv')\n    combined_data.to_csv(combined_csv_path, index=False)\n    print(f\"Data combination complete for {ticker}.\")\n    print(f\"Combined data saved to '{combined_csv_path}'\")",
        "detail": "data_ingestion.data_combiner",
        "documentation": {}
    },
    {
        "label": "fetch_beam_data",
        "kind": 2,
        "importPath": "data_ingestion.data_fetcher",
        "description": "data_ingestion.data_fetcher",
        "peekOfCode": "def fetch_beam_data(ticker, start_date, end_date, interval, api_key):\n    beam_url = 'https://api.beamapi.com/data/fundamentals/us/sec/form_4/v1'\n    transport = RequestsHTTPTransport(\n        url=beam_url,\n        headers={'Authorization': f'Bearer {api_key}'},\n        use_json=True,\n    )\n    try:\n        client = Client(transport=transport, fetch_schema_from_transport=True)\n        query = gql('''",
        "detail": "data_ingestion.data_fetcher",
        "documentation": {}
    },
    {
        "label": "fetch_alpha_vantage_data",
        "kind": 2,
        "importPath": "data_ingestion.data_fetcher",
        "description": "data_ingestion.data_fetcher",
        "peekOfCode": "def fetch_alpha_vantage_data(ticker, start_date, end_date, interval):\n    try:\n        ts = TimeSeries(key=config.ALPHA_VANTAGE_API_KEY)\n        data, _ = ts.get_daily(symbol=ticker, outputsize='full')\n        df = pd.DataFrame(data).T\n        df.index = pd.to_datetime(df.index)\n        df = df[(df.index >= start_date) & (df.index <= end_date)]\n        df.reset_index(inplace=True)\n        df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n        df['adj_close'] = df['close']",
        "detail": "data_ingestion.data_fetcher",
        "documentation": {}
    },
    {
        "label": "save_data",
        "kind": 2,
        "importPath": "data_ingestion.data_fetcher",
        "description": "data_ingestion.data_fetcher",
        "peekOfCode": "def save_data(data, ticker, source):\n    if data is not None and not data.empty:\n        output_dir = get_data_path(__file__, 'csv')\n        os.makedirs(output_dir, exist_ok=True)\n        output_file = os.path.join(output_dir, f'{ticker}_{source}_data.csv')\n        data.to_csv(output_file, index=False)\n        logger.info(f\"{source.capitalize()} data saved to {output_file}\")\n    else:\n        logger.warning(f\"No data to save for {ticker} from {source}\")\ndef main(ticker, start_date, end_date, interval, api_sources, beam_api_key):",
        "detail": "data_ingestion.data_fetcher",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "data_ingestion.data_fetcher",
        "description": "data_ingestion.data_fetcher",
        "peekOfCode": "def main(ticker, start_date, end_date, interval, api_sources, beam_api_key):\n    data_fetched = False\n    for source in api_sources:\n        if source == 'beam':\n            beam_data = fetch_beam_data(ticker, start_date, end_date, interval, beam_api_key)\n            if beam_data is not None:\n                save_data(beam_data, ticker, 'beam')\n                data_fetched = True\n            else:\n                logger.warning(\"Falling back to Alpha Vantage due to Beam API failure\")",
        "detail": "data_ingestion.data_fetcher",
        "documentation": {}
    },
    {
        "label": "current_dir",
        "kind": 5,
        "importPath": "data_ingestion.data_fetcher",
        "description": "data_ingestion.data_fetcher",
        "peekOfCode": "current_dir = os.path.dirname(__file__)\n# Add the path to the data_cleaning directory\nsys.path.insert(0, os.path.join(current_dir, '../data_cleaning'))\n# Now you can import data_cleaner\nfrom data_cleaner import clean_data\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\ndef fetch_beam_data(ticker, start_date, end_date, interval, api_key):\n    beam_url = 'https://api.beamapi.com/data/fundamentals/us/sec/form_4/v1'\n    transport = RequestsHTTPTransport(",
        "detail": "data_ingestion.data_fetcher",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "data_ingestion.data_fetcher",
        "description": "data_ingestion.data_fetcher",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef fetch_beam_data(ticker, start_date, end_date, interval, api_key):\n    beam_url = 'https://api.beamapi.com/data/fundamentals/us/sec/form_4/v1'\n    transport = RequestsHTTPTransport(\n        url=beam_url,\n        headers={'Authorization': f'Bearer {api_key}'},\n        use_json=True,\n    )\n    try:\n        client = Client(transport=transport, fetch_schema_from_transport=True)",
        "detail": "data_ingestion.data_fetcher",
        "documentation": {}
    },
    {
        "label": "get_project_path",
        "kind": 2,
        "importPath": "data_ingestion.path_utility",
        "description": "data_ingestion.path_utility",
        "peekOfCode": "def get_project_path(file_path):\n    \"\"\"\n    Get the project path based on the current file's location.\n    :param file_path: The __file__ variable of the current script\n    :return: Path object pointing to the project root (nxmbers folder)\n    \"\"\"\n    current_file = Path(file_path).resolve()\n    project_root = current_file.parent\n    while project_root.name != 'nxmbers':\n        project_root = project_root.parent",
        "detail": "data_ingestion.path_utility",
        "documentation": {}
    },
    {
        "label": "get_data_path",
        "kind": 2,
        "importPath": "data_ingestion.path_utility",
        "description": "data_ingestion.path_utility",
        "peekOfCode": "def get_data_path(file_path, filename):\n    \"\"\"\n    Get the full path for a data file within the project.\n    :param file_path: The __file__ variable of the current script\n    :param filename: Name of the file to be saved/loaded\n    :return: Full path to the data file\n    \"\"\"\n    project_root = get_project_path(file_path)\n    data_dir = project_root / 'nxmbers' / 'data'\n    data_dir.mkdir(parents=True, exist_ok=True)",
        "detail": "data_ingestion.path_utility",
        "documentation": {}
    },
    {
        "label": "get_project_path",
        "kind": 2,
        "importPath": "data_storage.path_utility",
        "description": "data_storage.path_utility",
        "peekOfCode": "def get_project_path(file_path):\n    \"\"\"\n    Get the project path based on the current file's location.\n    :param file_path: The __file__ variable of the current script\n    :return: Path object pointing to the project root (nxmbers folder)\n    \"\"\"\n    current_file = Path(file_path).resolve()\n    project_root = current_file.parent\n    while project_root.name != 'nxmbers':\n        project_root = project_root.parent",
        "detail": "data_storage.path_utility",
        "documentation": {}
    },
    {
        "label": "get_data_path",
        "kind": 2,
        "importPath": "data_storage.path_utility",
        "description": "data_storage.path_utility",
        "peekOfCode": "def get_data_path(file_path, filename):\n    \"\"\"\n    Get the full path for a data file within the project.\n    :param file_path: The __file__ variable of the current script\n    :param filename: Name of the file to be saved/loaded\n    :return: Full path to the data file\n    \"\"\"\n    project_root = get_project_path(file_path)\n    data_dir = project_root / 'nxmbers' / 'data'\n    data_dir.mkdir(parents=True, exist_ok=True)",
        "detail": "data_storage.path_utility",
        "documentation": {}
    },
    {
        "label": "create_table_name",
        "kind": 2,
        "importPath": "data_storage.rds_uploader",
        "description": "data_storage.rds_uploader",
        "peekOfCode": "def create_table_name():\n    \"\"\"Create a table name with current date.\"\"\"\n    return f\"market_data_{datetime.now().strftime('%Y_%m_%d_%H%M%S')}\"\ndef get_column_types(df):\n    \"\"\"Map pandas dtypes to PostgreSQL column types.\"\"\"\n    type_mapping = {\n        'int64': 'INTEGER',\n        'float64': 'FLOAT',\n        'object': 'TEXT',\n        'datetime64[ns]': 'TIMESTAMP'",
        "detail": "data_storage.rds_uploader",
        "documentation": {}
    },
    {
        "label": "get_column_types",
        "kind": 2,
        "importPath": "data_storage.rds_uploader",
        "description": "data_storage.rds_uploader",
        "peekOfCode": "def get_column_types(df):\n    \"\"\"Map pandas dtypes to PostgreSQL column types.\"\"\"\n    type_mapping = {\n        'int64': 'INTEGER',\n        'float64': 'FLOAT',\n        'object': 'TEXT',\n        'datetime64[ns]': 'TIMESTAMP'\n    }\n    return {col: type_mapping.get(str(df[col].dtype), 'TEXT') for col in df.columns}\ndef create_table(conn, cursor, table_name, column_types):",
        "detail": "data_storage.rds_uploader",
        "documentation": {}
    },
    {
        "label": "create_table",
        "kind": 2,
        "importPath": "data_storage.rds_uploader",
        "description": "data_storage.rds_uploader",
        "peekOfCode": "def create_table(conn, cursor, table_name, column_types):\n    \"\"\"Create a new table in the database.\"\"\"\n    columns = [\n        sql.SQL(\"{} {}\").format(\n            sql.Identifier(col),\n            sql.SQL(col_type)\n        ) for col, col_type in column_types.items()\n    ]\n    create_table_query = sql.SQL(\"CREATE TABLE IF NOT EXISTS {} ({})\").format(\n        sql.Identifier(table_name),",
        "detail": "data_storage.rds_uploader",
        "documentation": {}
    },
    {
        "label": "insert_data",
        "kind": 2,
        "importPath": "data_storage.rds_uploader",
        "description": "data_storage.rds_uploader",
        "peekOfCode": "def insert_data(conn, cursor, table_name, df):\n    \"\"\"Insert data into the table.\"\"\"\n    columns = list(df.columns)\n    values = [tuple(x) for x in df.to_numpy()]\n    insert_query = sql.SQL(\"INSERT INTO {} ({}) VALUES ({})\").format(\n        sql.Identifier(table_name),\n        sql.SQL(', ').join(map(sql.Identifier, columns)),\n        sql.SQL(', ').join(sql.Placeholder() * len(columns))\n    )\n    for value in tqdm(values, desc=\"Inserting data\"):",
        "detail": "data_storage.rds_uploader",
        "documentation": {}
    },
    {
        "label": "upload_to_rds",
        "kind": 2,
        "importPath": "data_storage.rds_uploader",
        "description": "data_storage.rds_uploader",
        "peekOfCode": "def upload_to_rds(file_path):\n    # Read the CSV file\n    df = pd.read_csv(file_path)\n    # Ensure 'date' column is datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # Connect to the database\n    conn = psycopg2.connect(\n        host=DB_HOST,\n        database=DB_NAME,\n        user=DB_USER,",
        "detail": "data_storage.rds_uploader",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "data_storage.rds_uploader",
        "description": "data_storage.rds_uploader",
        "peekOfCode": "def main():\n    # Define the directory containing the cleaned CSV files\n    cleaned_dir = os.path.join(os.path.dirname(__file__), '..', 'nxmbers', 'data', 'cleaned')\n    # Iterate over the cleaned CSV files\n    for file in os.listdir(cleaned_dir):\n        if file.endswith('.csv'):\n            file_path = os.path.join(cleaned_dir, file)\n            print(f\"Uploading {file} to RDS...\")\n            upload_to_rds(file_path)\nif __name__ == \"__main__\":",
        "detail": "data_storage.rds_uploader",
        "documentation": {}
    },
    {
        "label": "DB_HOST",
        "kind": 5,
        "importPath": "data_storage.rds_uploader",
        "description": "data_storage.rds_uploader",
        "peekOfCode": "DB_HOST = \"numbermxchine.cxwoaq8ccu34.eu-west-2.rds.amazonaws.com\"\nDB_NAME = \"nxmbers\"\nDB_USER = \"mxchinist\"\nDB_PASS = \"foJzyn-miwhor-bavpo4\"\nDB_PORT = \"5432\"\ndef create_table_name():\n    \"\"\"Create a table name with current date.\"\"\"\n    return f\"market_data_{datetime.now().strftime('%Y_%m_%d_%H%M%S')}\"\ndef get_column_types(df):\n    \"\"\"Map pandas dtypes to PostgreSQL column types.\"\"\"",
        "detail": "data_storage.rds_uploader",
        "documentation": {}
    },
    {
        "label": "DB_NAME",
        "kind": 5,
        "importPath": "data_storage.rds_uploader",
        "description": "data_storage.rds_uploader",
        "peekOfCode": "DB_NAME = \"nxmbers\"\nDB_USER = \"mxchinist\"\nDB_PASS = \"foJzyn-miwhor-bavpo4\"\nDB_PORT = \"5432\"\ndef create_table_name():\n    \"\"\"Create a table name with current date.\"\"\"\n    return f\"market_data_{datetime.now().strftime('%Y_%m_%d_%H%M%S')}\"\ndef get_column_types(df):\n    \"\"\"Map pandas dtypes to PostgreSQL column types.\"\"\"\n    type_mapping = {",
        "detail": "data_storage.rds_uploader",
        "documentation": {}
    },
    {
        "label": "DB_USER",
        "kind": 5,
        "importPath": "data_storage.rds_uploader",
        "description": "data_storage.rds_uploader",
        "peekOfCode": "DB_USER = \"mxchinist\"\nDB_PASS = \"foJzyn-miwhor-bavpo4\"\nDB_PORT = \"5432\"\ndef create_table_name():\n    \"\"\"Create a table name with current date.\"\"\"\n    return f\"market_data_{datetime.now().strftime('%Y_%m_%d_%H%M%S')}\"\ndef get_column_types(df):\n    \"\"\"Map pandas dtypes to PostgreSQL column types.\"\"\"\n    type_mapping = {\n        'int64': 'INTEGER',",
        "detail": "data_storage.rds_uploader",
        "documentation": {}
    },
    {
        "label": "DB_PASS",
        "kind": 5,
        "importPath": "data_storage.rds_uploader",
        "description": "data_storage.rds_uploader",
        "peekOfCode": "DB_PASS = \"foJzyn-miwhor-bavpo4\"\nDB_PORT = \"5432\"\ndef create_table_name():\n    \"\"\"Create a table name with current date.\"\"\"\n    return f\"market_data_{datetime.now().strftime('%Y_%m_%d_%H%M%S')}\"\ndef get_column_types(df):\n    \"\"\"Map pandas dtypes to PostgreSQL column types.\"\"\"\n    type_mapping = {\n        'int64': 'INTEGER',\n        'float64': 'FLOAT',",
        "detail": "data_storage.rds_uploader",
        "documentation": {}
    },
    {
        "label": "DB_PORT",
        "kind": 5,
        "importPath": "data_storage.rds_uploader",
        "description": "data_storage.rds_uploader",
        "peekOfCode": "DB_PORT = \"5432\"\ndef create_table_name():\n    \"\"\"Create a table name with current date.\"\"\"\n    return f\"market_data_{datetime.now().strftime('%Y_%m_%d_%H%M%S')}\"\ndef get_column_types(df):\n    \"\"\"Map pandas dtypes to PostgreSQL column types.\"\"\"\n    type_mapping = {\n        'int64': 'INTEGER',\n        'float64': 'FLOAT',\n        'object': 'TEXT',",
        "detail": "data_storage.rds_uploader",
        "documentation": {}
    },
    {
        "label": "update_config_data",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def update_config_data(ticker, start_date, end_date, interval, api_sources, beam_api_key):\n    # Fetch data\n    fetch_data(\n        ticker=ticker,\n        start_date=start_date,\n        end_date=end_date,\n        interval=interval,\n        api_sources=api_sources,\n        beam_api_key=beam_api_key\n    )",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "update_config",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def update_config():\n    data = request.json\n    ticker = data.get('ticker', USER_TICKER)\n    start_date = data.get('startDate', USER_START_DATE)\n    end_date = data.get('endDate', USER_END_DATE)\n    interval = data.get('interval', USER_INTERVAL)\n    api_sources = data.get('apiSources', USER_API_SOURCES)\n    update_config_data(ticker, start_date, end_date, interval, api_sources, BEAM_API_KEY)\n    return jsonify({\"message\": \"Data updated successfully\"}), 200\n@app.route('/')",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def index():\n    return render_template('index.html', \n                           default_ticker=config.DEFAULT_TICKER,\n                           default_start_date=config.DEFAULT_START_DATE,\n                           default_end_date=config.DEFAULT_END_DATE,\n                           default_interval=config.DEFAULT_INTERVAL,\n                           default_api_sources=config.DEFAULT_API_SOURCES)\nif __name__ == '__main__':\n    app.run(debug=True)",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "current_dir",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "current_dir = os.path.dirname(__file__)\n# Add the necessary paths to sys.path\nsys.path.insert(0, os.path.join(current_dir, '../data_cleaning'))\nsys.path.insert(0, os.path.join(current_dir, '../data_storage'))\nsys.path.insert(0, os.path.join(current_dir, '../data_ingestion'))\n# Print out the sys.path variable\nprint(sys.path)\n# Import the necessary modules\nfrom data_ingestion.data_fetcher import main as fetch_data\nfrom data_cleaning.data_cleaner import clean_data  # Corrected import",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "app = Flask(__name__)\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\ndef update_config_data(ticker, start_date, end_date, interval, api_sources, beam_api_key):\n    # Fetch data\n    fetch_data(\n        ticker=ticker,\n        start_date=start_date,\n        end_date=end_date,\n        interval=interval,",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef update_config_data(ticker, start_date, end_date, interval, api_sources, beam_api_key):\n    # Fetch data\n    fetch_data(\n        ticker=ticker,\n        start_date=start_date,\n        end_date=end_date,\n        interval=interval,\n        api_sources=api_sources,\n        beam_api_key=beam_api_key",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "db_host",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "db_host = \"numbermxchine.cxwoaq8ccu34.eu-west-2.rds.amazonaws.com\"\ndb_name = \"nxmbers\"\ndb_user = \"mxchinist\"\ndb_password = \"foJzyn-miwhor-bavpo4\"\ndb_port = 5432\nforecast_horizon = 12\n# ... other variables\n# config.py\nALPHA_VANTAGE_API_KEY = \"KXUK213E0W7JLEQV\"\nBEAM_API_KEY = \"NmJkNzkxNGQtYzUwMy00YjAzLTkyMzYtODUxMmJlYWJmY2M1\"",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "db_name",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "db_name = \"nxmbers\"\ndb_user = \"mxchinist\"\ndb_password = \"foJzyn-miwhor-bavpo4\"\ndb_port = 5432\nforecast_horizon = 12\n# ... other variables\n# config.py\nALPHA_VANTAGE_API_KEY = \"KXUK213E0W7JLEQV\"\nBEAM_API_KEY = \"NmJkNzkxNGQtYzUwMy00YjAzLTkyMzYtODUxMmJlYWJmY2M1\"\nDEFAULT_TICKER = 'AAPL'",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "db_user",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "db_user = \"mxchinist\"\ndb_password = \"foJzyn-miwhor-bavpo4\"\ndb_port = 5432\nforecast_horizon = 12\n# ... other variables\n# config.py\nALPHA_VANTAGE_API_KEY = \"KXUK213E0W7JLEQV\"\nBEAM_API_KEY = \"NmJkNzkxNGQtYzUwMy00YjAzLTkyMzYtODUxMmJlYWJmY2M1\"\nDEFAULT_TICKER = 'AAPL'\nDEFAULT_START_DATE = '2000-01-01'",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "db_password",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "db_password = \"foJzyn-miwhor-bavpo4\"\ndb_port = 5432\nforecast_horizon = 12\n# ... other variables\n# config.py\nALPHA_VANTAGE_API_KEY = \"KXUK213E0W7JLEQV\"\nBEAM_API_KEY = \"NmJkNzkxNGQtYzUwMy00YjAzLTkyMzYtODUxMmJlYWJmY2M1\"\nDEFAULT_TICKER = 'AAPL'\nDEFAULT_START_DATE = '2000-01-01'\nDEFAULT_END_DATE = '2024-08-01'",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "db_port",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "db_port = 5432\nforecast_horizon = 12\n# ... other variables\n# config.py\nALPHA_VANTAGE_API_KEY = \"KXUK213E0W7JLEQV\"\nBEAM_API_KEY = \"NmJkNzkxNGQtYzUwMy00YjAzLTkyMzYtODUxMmJlYWJmY2M1\"\nDEFAULT_TICKER = 'AAPL'\nDEFAULT_START_DATE = '2000-01-01'\nDEFAULT_END_DATE = '2024-08-01'\nDEFAULT_INTERVAL = '1day'",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "forecast_horizon",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "forecast_horizon = 12\n# ... other variables\n# config.py\nALPHA_VANTAGE_API_KEY = \"KXUK213E0W7JLEQV\"\nBEAM_API_KEY = \"NmJkNzkxNGQtYzUwMy00YjAzLTkyMzYtODUxMmJlYWJmY2M1\"\nDEFAULT_TICKER = 'AAPL'\nDEFAULT_START_DATE = '2000-01-01'\nDEFAULT_END_DATE = '2024-08-01'\nDEFAULT_INTERVAL = '1day'\nDEFAULT_API_SOURCES = ['beam', 'alpha']",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "ALPHA_VANTAGE_API_KEY",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "ALPHA_VANTAGE_API_KEY = \"KXUK213E0W7JLEQV\"\nBEAM_API_KEY = \"NmJkNzkxNGQtYzUwMy00YjAzLTkyMzYtODUxMmJlYWJmY2M1\"\nDEFAULT_TICKER = 'AAPL'\nDEFAULT_START_DATE = '2000-01-01'\nDEFAULT_END_DATE = '2024-08-01'\nDEFAULT_INTERVAL = '1day'\nDEFAULT_API_SOURCES = ['beam', 'alpha']\n# User input values (to be updated by the frontend)\nUSER_TICKER = DEFAULT_TICKER\nUSER_START_DATE = DEFAULT_START_DATE",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "BEAM_API_KEY",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "BEAM_API_KEY = \"NmJkNzkxNGQtYzUwMy00YjAzLTkyMzYtODUxMmJlYWJmY2M1\"\nDEFAULT_TICKER = 'AAPL'\nDEFAULT_START_DATE = '2000-01-01'\nDEFAULT_END_DATE = '2024-08-01'\nDEFAULT_INTERVAL = '1day'\nDEFAULT_API_SOURCES = ['beam', 'alpha']\n# User input values (to be updated by the frontend)\nUSER_TICKER = DEFAULT_TICKER\nUSER_START_DATE = DEFAULT_START_DATE\nUSER_END_DATE = DEFAULT_END_DATE",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TICKER",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "DEFAULT_TICKER = 'AAPL'\nDEFAULT_START_DATE = '2000-01-01'\nDEFAULT_END_DATE = '2024-08-01'\nDEFAULT_INTERVAL = '1day'\nDEFAULT_API_SOURCES = ['beam', 'alpha']\n# User input values (to be updated by the frontend)\nUSER_TICKER = DEFAULT_TICKER\nUSER_START_DATE = DEFAULT_START_DATE\nUSER_END_DATE = DEFAULT_END_DATE\nUSER_INTERVAL = DEFAULT_INTERVAL",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "DEFAULT_START_DATE",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "DEFAULT_START_DATE = '2000-01-01'\nDEFAULT_END_DATE = '2024-08-01'\nDEFAULT_INTERVAL = '1day'\nDEFAULT_API_SOURCES = ['beam', 'alpha']\n# User input values (to be updated by the frontend)\nUSER_TICKER = DEFAULT_TICKER\nUSER_START_DATE = DEFAULT_START_DATE\nUSER_END_DATE = DEFAULT_END_DATE\nUSER_INTERVAL = DEFAULT_INTERVAL\nUSER_API_SOURCES = DEFAULT_API_SOURCES",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "DEFAULT_END_DATE",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "DEFAULT_END_DATE = '2024-08-01'\nDEFAULT_INTERVAL = '1day'\nDEFAULT_API_SOURCES = ['beam', 'alpha']\n# User input values (to be updated by the frontend)\nUSER_TICKER = DEFAULT_TICKER\nUSER_START_DATE = DEFAULT_START_DATE\nUSER_END_DATE = DEFAULT_END_DATE\nUSER_INTERVAL = DEFAULT_INTERVAL\nUSER_API_SOURCES = DEFAULT_API_SOURCES",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "DEFAULT_INTERVAL",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "DEFAULT_INTERVAL = '1day'\nDEFAULT_API_SOURCES = ['beam', 'alpha']\n# User input values (to be updated by the frontend)\nUSER_TICKER = DEFAULT_TICKER\nUSER_START_DATE = DEFAULT_START_DATE\nUSER_END_DATE = DEFAULT_END_DATE\nUSER_INTERVAL = DEFAULT_INTERVAL\nUSER_API_SOURCES = DEFAULT_API_SOURCES",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "DEFAULT_API_SOURCES",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "DEFAULT_API_SOURCES = ['beam', 'alpha']\n# User input values (to be updated by the frontend)\nUSER_TICKER = DEFAULT_TICKER\nUSER_START_DATE = DEFAULT_START_DATE\nUSER_END_DATE = DEFAULT_END_DATE\nUSER_INTERVAL = DEFAULT_INTERVAL\nUSER_API_SOURCES = DEFAULT_API_SOURCES",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_TICKER",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "USER_TICKER = DEFAULT_TICKER\nUSER_START_DATE = DEFAULT_START_DATE\nUSER_END_DATE = DEFAULT_END_DATE\nUSER_INTERVAL = DEFAULT_INTERVAL\nUSER_API_SOURCES = DEFAULT_API_SOURCES",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_START_DATE",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "USER_START_DATE = DEFAULT_START_DATE\nUSER_END_DATE = DEFAULT_END_DATE\nUSER_INTERVAL = DEFAULT_INTERVAL\nUSER_API_SOURCES = DEFAULT_API_SOURCES",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_END_DATE",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "USER_END_DATE = DEFAULT_END_DATE\nUSER_INTERVAL = DEFAULT_INTERVAL\nUSER_API_SOURCES = DEFAULT_API_SOURCES",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_INTERVAL",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "USER_INTERVAL = DEFAULT_INTERVAL\nUSER_API_SOURCES = DEFAULT_API_SOURCES",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "USER_API_SOURCES",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "USER_API_SOURCES = DEFAULT_API_SOURCES",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def main():\n    # Fetch data\n    update_config_data(\n        ticker=config.USER_TICKER,\n        start_date=config.USER_START_DATE,\n        end_date=config.USER_END_DATE,\n        interval=config.USER_INTERVAL,\n        api_sources=config.USER_API_SOURCES,\n        beam_api_key=config.BEAM_API_KEY\n    )",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "get_project_path",
        "kind": 2,
        "importPath": "path_utility",
        "description": "path_utility",
        "peekOfCode": "def get_project_path(file_path):\n    \"\"\"\n    Get the project path based on the current file's location.\n    :param file_path: The __file__ variable of the current script\n    :return: Path object pointing to the project root (nxmbers folder)\n    \"\"\"\n    current_file = Path(file_path).resolve()\n    project_root = current_file.parent\n    while project_root.name != 'nxmbers':\n        project_root = project_root.parent",
        "detail": "path_utility",
        "documentation": {}
    },
    {
        "label": "get_data_path",
        "kind": 2,
        "importPath": "path_utility",
        "description": "path_utility",
        "peekOfCode": "def get_data_path(file_path, filename):\n    \"\"\"\n    Get the full path for a data file within the project.\n    :param file_path: The __file__ variable of the current script\n    :param filename: Name of the file to be saved/loaded\n    :return: Full path to the data file\n    \"\"\"\n    project_root = get_project_path(file_path)\n    data_dir = project_root / 'nxmbers' / 'data'\n    data_dir.mkdir(parents=True, exist_ok=True)",
        "detail": "path_utility",
        "documentation": {}
    },
    {
        "label": "print_directory_tree",
        "kind": 2,
        "importPath": "tree",
        "description": "tree",
        "peekOfCode": "def print_directory_tree(path, prefix=\"\"):\n    \"\"\"Recursively prints a directory tree, ignoring hidden files.\"\"\"\n    entries = [entry for entry in os.listdir(path) if not entry.startswith('.')]\n    entries.sort() \n    num_entries = len(entries)\n    for index, entry in enumerate(entries):\n        entry_path = os.path.join(path, entry)\n        if os.path.isdir(entry_path):\n            # Directory\n            connector = \"└── \" if index == num_entries - 1 else \"├── \"",
        "detail": "tree",
        "documentation": {}
    },
    {
        "label": "start_path",
        "kind": 5,
        "importPath": "tree",
        "description": "tree",
        "peekOfCode": "start_path = \".\"\nprint_directory_tree(start_path)",
        "detail": "tree",
        "documentation": {}
    }
]